# HiBench: K-means

This workload from the HiBench suite tests K-means clustering in `spark.mllib`, a well-known clustering algorithm for knowledge discovery and data mining. The input data set is generated by `GenKMeansDataset` based on Uniform Distribution and Gaussian Distribution. There is also an optimized K-means implementation based on Intel Data Analytics Library (DAL), which is available in the `dal` module of sparkbench.

This benchmark requires Spark 3.3.0, HiBench 7.1.1, and Hadoop 3.3.6.  HiBench 7.1.1 also requires JDK 8.0, Spark 2.4, and Scale 2.11.
HiBench is the workload generator, Hadoop is used to generate and store data, and Spark is the application we wish to test.



## Installation

### HiBench

HiBench is an open source benchmark for big data applications. It works well with latest Spark and Hadoop version, but in order to install and compile it, we need to use Java 8. Newer Java versions are not compatible. In order to make maven work, you have to use `-Dspark=2.4 -Dscala=2.11` as shown below. After the installation is done, you can switch to another version of Spark in your `conf/spark.conf` file. 

```bash
sudo apt install openjdk-11-jre-headless openjdk-11-jdk-headless maven python2 net-tools openjdk-8-jdk
 
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-arm64"
export PATH="$JAVA_HOME/bin:$PATH"
 
git clone https://github.com/Intel-bigdata/HiBench.git
cd HiBench
mvn -Phadoopbench -Psparkbench -Dspark=2.4 -Dscala=2.11 clean package

cd conf
cp hadoop.conf.template hadoop.conf  # you want to revisit after your hadoop is installed and running
cp spark.conf.template spark.conf    # you want to revisit after your spark is installed and running
```

### Hadoop

Apache Hadoop 3.3.0 contains a number of significant features and enhancements. A few of them are noted as below.
This is the first release to support Arm64 CPUs, and Java 11 runtime support is completed.

```bash
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
tar zxvf hadoop-3.3.6.tar.gz
cd ./hadoop-3.3.6/etc/hadoop
```

Modify the following files:

* `yarn-site.xml`

    ```xml
    <configuration>
    <!-- Site specific YARN configuration properties -->
    <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
    </property>
    <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>127.0.0.1</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>127.0.0.1:8032</value>
    </property>
    <property>
      <name>yarn.resourcemanager.scheduler.address</name>
      <value>127.0.0.1:8030</value>
    </property>
    <property>
      <name>yarn.resourcemanager.resource-tracker.address</name>
      <value>127.0.0.1:8031</value>
    </property>
    <property>
    <name>yarn.nodemanager.vmem-pmem-ratio</name>
    <value>5</value>
    </property>
    </configuration>
    ```

* `core-site.xml`

    ```xml
    <configuration>
    <property>
    <name>fs.default.name</name>
    <value>hdfs://localhost:8020</value>
    </property> 
    </configuration>
    ```

* `core-site.xml`

    ```xml
    <configuration>
    <property>
    <name>fs.default.name</name>
    <value>hdfs://localhost:8020</value>
    </property> 
    </configuration>
    ```

* `mapred-site.xml`

    ```xml
    <configuration>
    <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
    </property>
    <property>
      <name>yarn.app.mapreduce.am.env</name>
      <value>HADOOP_MAPRED_HOME=$PATH_TO_HADOOP</value>
    </property>
    <property>
      <name>mapreduce.map.env</name>
      <value>HADOOP_MAPRED_HOME=$PATH_TO_HADOOP</value>
    </property>
    <property>
      <name>mapreduce.reduce.env</name>
      <value>HADOOP_MAPRED_HOME=$PATH_TO_HADOOP</value>
    </property>
    </configuration>
    ```

* `hdfs-site.xml`

    ```xml
    <configuration>
    <property>
    <name>dfs.replication</name>
    <value>1</value>
    </property>
    <property>
    <name>dfs.namenode.name.dir</name>
    <value>$PATH_TO_HADOOP/namenode</value>
    </property>
    <property>
    <name>dfs.datanode.data.dir</name>
    <value>datanode</value>
    </property>
    </configuration>
    ```

* `hadoop-env.sh`

    ```bash
    export JAVA_HOME="/usr/lib/jvm/java-11-openjdk-arm64"  
    # Location of Hadoop.  By default, Hadoop will attempt to determine
    # this location based upon its execution path.
    export HADOOP_HOME="$PATH_TO_HADOOP"
      
    # Location of Hadoop's configuration information.  i.e., where this
    # file is living. If this is not defined, Hadoop will attempt to
    # locate it based upon its execution path.
    #
    # NOTE: It is recommend that this variable not be set here but in
    # /etc/profile.d or equivalent.  Some options (such as
    # --config) may react strangely otherwise.
    #
    export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
    ```

### Spark

Set the following environment variables

```bash
export JAVA_HOME="/usr/lib/jvm/java-11-openjdk-arm64"
export PATH="$JAVA_HOME/bin:$PATH"
  
# Set Hadoop-related environment variables
export HADOOP_PREFIX=$PATH_TO_HADOOP
export HADOOP_HOME=$PATH_TO_HADOOP
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export YARN_HOME=${HADOOP_HOME}
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
# Native Path
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_PREFIX/lib"
# Add Hadoop bin/ directory to PATH
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

Install spark

```bash
wget https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz
tar zxvf spark-3.3.0-bin-hadoop3.tgz
cd spark-3.3.0-bin-hadoop3/conf
cp spark-env.sh.template spark-env.sh
cp spark-defaults.conf.template spark-defaults.conf
```

Make changes to the following files based on your installation paths for Hadoop, Spark and HiBench.

* `hadoop.conf`

    ```bash
    # Hadoop home
    hibench.hadoop.home    $PATH_TO_HADOOP
    
    # The path of hadoop executable
    hibench.hadoop.executable     ${hibench.hadoop.home}/bin/hadoop
    
    # Hadoop configraution directory
    hibench.hadoop.configure.dir  ${hibench.hadoop.home}/etc/hadoop
    
    # The root HDFS path to store HiBench data
    hibench.hdfs.master       hdfs://localhost:8020
    
    
    # Hadoop release provider. Supported value: apache
    hibench.hadoop.release    apache
    ```

* `spark.conf`

    ```bash
    # Spark home
    hibench.spark.home      $PATH_TO_SPARK
    
    # Spark master
    #   standalone mode: spark://xxx:7077
    #   YARN mode: yarn-client
    hibench.spark.master    local[*]
    
    # executor number and cores when running on Yarn
    hibench.yarn.executor.num     2
    hibench.yarn.executor.cores   4
    
    # executor and driver memory in standalone & YARN mode
    spark.executor.memory  50g
    spark.driver.memory    50g
    
    # set spark parallelism property according to hibench's parallelism value
    spark.default.parallelism     ${hibench.default.map.parallelism}
    
    # set spark sql's default shuffle partitions according to hibench's parallelism value
    spark.sql.shuffle.partitions  ${hibench.default.shuffle.parallelism}
    
    #======================================================
    # Spark Streaming
    #======================================================
    # Spark streaming Batchnterval in millisecond (default 100)
    hibench.streambench.spark.batchInterval          100
    
    # Number of nodes that will receive kafka input (default: 4)
    hibench.streambench.spark.receiverNumber        4
    
    # Indicate RDD storage level. (default: 2)
    # 0 = StorageLevel.MEMORY_ONLY
    # 1 = StorageLevel.MEMORY_AND_DISK_SER
    # other = StorageLevel.MEMORY_AND_DISK_SER_2
    hibench.streambench.spark.storageLevel 2
    
    # indicate whether to test the write ahead log new feature (default: false)
    hibench.streambench.spark.enableWAL false
    
    # if testWAL is true, this path to store stream context in hdfs shall be specified. If false, it can be empty (default: /var/tmp)
    hibench.streambench.spark.checkpointPath /var/tmp
    
    # whether to use direct approach or not (dafault: true)
    hibench.streambench.spark.useDirectMode true
    ```

* `hibench.conf`

    ```bash
    # Data scale profile. Available value is tiny, small, large, huge, gigantic and bigdata.
    # The definition of these profiles can be found in the workload's conf file i.e. conf/workloads/micro/wordcount.conf
    hibench.scale.profile                huge
    # Mapper number in hadoop, partition number in Spark
    hibench.default.map.parallelism         128
    
    # Reducer nubmer in hadoop, shuffle partition number in Spark
    hibench.default.shuffle.parallelism     128
    
    
    #======================================================
    # Report files
    #======================================================
    # default report formats
    hibench.report.formats      "%-12s %-10s %-8s %-20s %-20s %-20s %-20s\n"
    
    # default report dir path
    hibench.report.dir      ${hibench.home}/report
    
    # default report file name
    hibench.report.name     hibench.report
    
    # input/output format settings. Available formats: Text, Sequence, Null.
    sparkbench.inputformat         Sequence
    sparkbench.outputformat        Sequence
    
    # hibench config folder
    hibench.configure.dir       ${hibench.home}/conf
    
    # default hibench HDFS root
    hibench.hdfs.data.dir       ${hibench.hdfs.master}/HiBench
    
    # path of hibench jars
    hibench.hibench.datatool.dir              ${hibench.home}/autogen/target/autogen-8.0-SNAPSHOT-jar-with-dependencies.jar
    hibench.common.jar                      ${hibench.home}/common/target/hibench-common-8.0-SNAPSHOT-jar-with-dependencies.jar
    hibench.sparkbench.jar                  ${hibench.home}/sparkbench/assembly/target/sparkbench-assembly-8.0-SNAPSHOT-dist.jar
    hibench.streambench.stormbench.jar      ${hibench.home}/stormbench/streaming/target/stormbench-streaming-8.0-SNAPSHOT.jar
    hibench.streambench.gearpump.jar        ${hibench.home}/gearpumpbench/streaming/target/gearpumpbench-streaming-8.0-SNAPSHOT-jar-with-dependencies.jar
    hibench.streambench.flinkbench.jar      ${hibench.home}/flinkbench/streaming/target/flinkbench-streaming-8.0-SNAPSHOT-jar-with-dependencies.jar
    
    #======================================================
    # workload home/input/ouput path
    #======================================================
    hibench.hive.home       ${hibench.home}/hadoopbench/sql/target/${hibench.hive.release}
    hibench.hive.release        apache-hive-3.0.0-bin
    hibench.hivebench.template.dir  ${hibench.home}/hadoopbench/sql/hive_template
    hibench.bayes.dir.name.input    ${hibench.workload.dir.name.input}
    hibench.bayes.dir.name.output   ${hibench.workload.dir.name.output}
    
    hibench.mahout.release.apache   apache-mahout-distribution-0.11.0
    hibench.mahout.release            ${hibench.mahout.release.${hibench.hadoop.release}}
    hibench.mahout.home               ${hibench.home}/hadoopbench/mahout/target/${hibench.mahout.release}
    
    hibench.masters.hostnames
    hibench.slaves.hostnames
    
    hibench.workload.input
    hibench.workload.output
    hibench.workload.dir.name.input         Input
    hibench.workload.dir.name.output        Output
    
    hibench.nutch.dir.name.input    ${hibench.workload.dir.name.input}
    hibench.nutch.dir.name.output   ${hibench.workload.dir.name.output}
    hibench.nutch.nutchindexing.dir ${hibench.home}/hadoopbench/nutchindexing/
    hibench.nutch.release       nutch-1.2
    hibench.nutch.home      ${hibench.home}/hadoopbench/nutchindexing/target/${hibench.nutch.release}
    
    hibench.dfsioe.dir.name.input   ${hibench.workload.dir.name.input}
    hibench.dfsioe.dir.name.output  ${hibench.workload.dir.name.output}
    
    
    #======================================================
    # Streaming General
    #======================================================
    # Indicate whether in debug mode for correctness verfication (default: false)
    hibench.streambench.debugMode false
    hibench.streambench.sampleProbability 0.1
    hibench.streambench.fixWindowDuration            10000
    hibench.streambench.fixWindowSlideStep           10000
    
    
    #======================================================
    # Kafka for streaming benchmarks
    #======================================================
    hibench.streambench.kafka.home                  /PATH/TO/YOUR/KAFKA/HOME
    # zookeeper host:port of kafka cluster, host1:port1,host2:port2...
    hibench.streambench.zkHost
    # Kafka broker lists, written in mode host:port,host:port,..
    hibench.streambench.kafka.brokerList
    hibench.streambench.kafka.consumerGroup          HiBench
    # number of partitions of generated topic (default 20)
    hibench.streambench.kafka.topicPartitions       20
    # consumer group of the consumer for kafka (default: HiBench)
    hibench.streambench.kafka.consumerGroup HiBench
    # Set the starting offset of kafkaConsumer (default: largest)
    hibench.streambench.kafka.offsetReset largest
    
    
    #======================================================
    # Data generator for streaming benchmarks
    #======================================================
    # Interval span in millisecond (default: 50)
    hibench.streambench.datagen.intervalSpan         50
    # Number of records to generate per interval span (default: 5)
    hibench.streambench.datagen.recordsPerInterval   5
    # fixed length of record (default: 200)
    hibench.streambench.datagen.recordLength         200
    # Number of KafkaProducer running on different thread (default: 1)
    hibench.streambench.datagen.producerNumber       1
    # Total round count of data send (default: -1 means infinity)
    hibench.streambench.datagen.totalRounds          -1
    # Number of total records that will be generated (default: -1 means infinity)
    hibench.streambench.datagen.totalRecords        -1
    # default path to store seed files (default: ${hibench.hdfs.data.dir}/Streaming)
    hibench.streambench.datagen.dir                         ${hibench.hdfs.data.dir}/Streaming
    # default path setting for genearate data1 & data2
    hibench.streambench.datagen.data1.name                  Seed
    hibench.streambench.datagen.data1.dir                   ${hibench.streambench.datagen.dir}/${hibench.streambench.datagen.data1.name}
    hibench.streambench.datagen.data2_cluster.dir           ${hibench.streambench.datagen.dir}/Kmeans/Cluster
    hibench.streambench.datagen.data2_samples.dir           ${hibench.streambench.datagen.dir}/Kmeans/Samples
    
    #======================================================
    # MetricsReader for streaming benchmarks
    #======================================================
    # Number of sample records for `MetricsReader` (default: 5000000)
    hibench.streambench.metricsReader.sampleNum      5000000
    # Number of thread for `MetricsReader` (default: 20)
    hibench.streambench.metricsReader.threadNum      20
    # The dir where stored the report of benchmarks (default: ${hibench.home}/report)
    hibench.streambench.metricsReader.outputDir      ${hibench.home}/report
    ```


## Running the Benchmark

1. Start Hadoop
    
    ```bash
    $PATH_TO_HADOOP/sbin/start-all.sh
    ```
    
    If Step 1 was successful, here is the sample output:

    ```
    <PROMPT>$ jps
    369207 SecondaryNameNode
    369985 NodeManager
    371293 NameNode
    373148 Jps
    368895 DataNode
    369529 ResourceManager
    ```

2. Run the following script to run Spark k-means on a 72 core Grace system.

    ```bash
    #!/usr/bin/bash
    # This scripts assumes user launched hadoop successfully. That is, command "jps" shows result similar as below:
    #369207 SecondaryNameNode
    #369985 NodeManager
    #371293 NameNode
    #373148 Jps
    #368895 DataNode
    #369529 ResourceManager

    #Change to conf file path to the one in your environment
    conf=$PATH_TO_HiBench/conf/hibench.conf
    thread=72

    echo "thread is $thread"
    sed -i "s/hibench.default.map.parallelism         [[:digit:]]\+/hibench.default.map.parallelism         $thread/1" $conf
    sed -i "s/hibench.default.shuffle.parallelism     [[:digit:]]\+/hibench.default.shuffle.parallelism     $thread/1" $conf
    
    sleep 1
    #Change the path to prepare.sh below to your specific installation
    $PATH_TO_HiBench/bin/workloads/ml/kmeans/prepare/prepare.sh
    sleep 1
    #Change the path to run.sh below to your specific installation 
    numactl -N0 -m0 taskset -a -c 0-71 $PATH_TO_HiBench/bin/workloads/ml/kmeans/spark/run.sh
    sleep 1
    numactl -N0 -m0 taskset -a -c 0-71 $PATH_TO_HiBench/bin/workloads/ml/kmeans/spark/run.sh
    sleep 1
    numactl -N0 -m0 taskset -a -c 0-71 $PATH_TO_HiBench/bin/workloads/ml/kmeans/spark/run.sh
    sleep 1
    echo "$thread is done\n"
    ```

The results can be found in the file `$HIBENCH_DIR/report/hibench.report`.

## Reference Results

```admonish important 
These figures are provided as guidelines and should not be interpreted as performance targets.
```


Here is the expected output on Grace:
```
Type         Date       Time     Input_data_size      Duration(s)          Throughput(bytes/s)  Throughput/node    
ScalaSparkKmeans 2023-02-16 20:29:54 19903891441          23.692               840110224            840110224          
ScalaSparkKmeans 2023-02-16 23:45:33 19903891427          23.742               838340974            838340974          
ScalaSparkKmeans 2023-02-16 23:53:05 19903891439          24.129               824894999            824894999
 
In the above case, the median throughput/node across the three runs is the result. In the above example, it is  838340974 (bytes/s)
```




